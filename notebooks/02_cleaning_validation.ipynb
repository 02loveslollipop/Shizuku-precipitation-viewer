{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c37a5f0",
   "metadata": {},
   "source": [
    "# Step 4 Â· Cleaning pipeline validation\n",
    "\n",
    "Validate the precipitation cleaning logic inside the notebook without relying on the service package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1da1c",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Run this notebook from the repo root so relative paths work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import sqlalchemy as sa\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc819c1",
   "metadata": {},
   "source": [
    "### Globals & Helper Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37164f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTLIER_FLAG = 1\n",
    "IMPUTED_FLAG = 2\n",
    "POOR_QUALITY_FLAG = 4\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class CleanerConfig:\n",
    "    min_value_mm: float = 0.0\n",
    "    max_value_mm: float = 150.0\n",
    "    min_quality: Optional[float] = None\n",
    "    interpolation_limit: int = 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee90d3",
   "metadata": {},
   "source": [
    "### Cleaning Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_measurements(raw_df: pd.DataFrame, cfg: CleanerConfig) -> pd.DataFrame:\n",
    "    if raw_df.empty:\n",
    "        return pd.DataFrame(columns=[\"sensor_id\", \"ts\", \"value_mm\", \"qc_flags\", \"imputation_method\", \"version\"])\n",
    "\n",
    "    results = []\n",
    "    for sensor_id, group in raw_df.groupby(\"sensor_id\"):\n",
    "        cleaned = _clean_sensor_dataframe(sensor_id, group, cfg)\n",
    "        if not cleaned.empty:\n",
    "            results.append(cleaned)\n",
    "    if not results:\n",
    "        return pd.DataFrame(columns=[\"sensor_id\", \"ts\", \"value_mm\", \"qc_flags\", \"imputation_method\", \"version\"])\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "def _clean_sensor_dataframe(sensor_id: str, df: pd.DataFrame, cfg: CleanerConfig) -> pd.DataFrame:\n",
    "    df = df.sort_values(\"ts\").reset_index(drop=True)\n",
    "    ts_index = pd.to_datetime(df[\"ts\"], utc=True)\n",
    "\n",
    "    values = pd.to_numeric(df[\"value_mm\"], errors=\"coerce\")\n",
    "    clean_series = pd.Series(values.to_numpy(dtype=float), index=ts_index, dtype=float)\n",
    "\n",
    "    qc_flags = np.zeros(len(df), dtype=np.int32)\n",
    "\n",
    "    outlier_mask = (clean_series < cfg.min_value_mm) | (clean_series > cfg.max_value_mm)\n",
    "    clean_series[outlier_mask] = np.nan\n",
    "    qc_flags[outlier_mask.to_numpy()] |= OUTLIER_FLAG\n",
    "\n",
    "    if cfg.min_quality is not None and \"quality\" in df:\n",
    "        quality = pd.to_numeric(df[\"quality\"], errors=\"coerce\")\n",
    "        poor_quality_mask = quality < cfg.min_quality\n",
    "        poor_quality_mask = poor_quality_mask.fillna(False)\n",
    "        if poor_quality_mask.any():\n",
    "            clean_series[poor_quality_mask.to_numpy()] = np.nan\n",
    "            qc_flags[poor_quality_mask.to_numpy()] |= POOR_QUALITY_FLAG\n",
    "\n",
    "    imputation_method = pd.Series(index=clean_series.index, dtype=\"object\")\n",
    "    missing_before = clean_series.isna()\n",
    "\n",
    "    if missing_before.any():\n",
    "        gbm_filled, labels = gbm_forecast_fill(clean_series, cfg)\n",
    "        clean_series = clean_series.copy()\n",
    "        clean_series.loc[missing_before] = gbm_filled.loc[missing_before]\n",
    "        imputation_method.loc[labels.notna()] = labels.loc[labels.notna()]\n",
    "    \n",
    "    remaining = clean_series.isna()\n",
    "    if remaining.any():\n",
    "        interp = clean_series.interpolate(method=\"time\", limit_direction=\"both\")\n",
    "        clean_series.loc[remaining & interp.notna()] = interp.loc[remaining & interp.notna()]\n",
    "        imputation_method.loc[remaining & interp.notna()] = \"time_interp\"\n",
    "\n",
    "    clean_series = clean_series.clip(lower=cfg.min_value_mm, upper=cfg.max_value_mm)\n",
    "\n",
    "    imputed_mask = imputation_method.notna()\n",
    "    qc_flags[imputed_mask.to_numpy()] |= IMPUTED_FLAG\n",
    "\n",
    "    valid_mask = ~clean_series.isna()\n",
    "    if not valid_mask.any():\n",
    "        return pd.DataFrame(columns=[\"sensor_id\", \"ts\", \"value_mm\", \"qc_flags\", \"imputation_method\", \"version\"])\n",
    "\n",
    "    qc_flags_series = pd.Series(qc_flags, index=clean_series.index)\n",
    "    imputation_method = imputation_method.where(imputation_method.notna(), None)\n",
    "\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            \"sensor_id\": sensor_id,\n",
    "            \"ts\": clean_series.index,\n",
    "            \"value_mm\": clean_series.values,\n",
    "            \"qc_flags\": qc_flags_series.values,\n",
    "            \"imputation_method\": imputation_method.values,\n",
    "            \"version\": 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = result[valid_mask.values].reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "def gbm_forecast_fill(series: pd.Series, cfg: CleanerConfig, random_state: int = 42) -> tuple[pd.Series, pd.Series]:\n",
    "    series = series.copy().astype(float)\n",
    "    labels = pd.Series(index=series.index, dtype=\"object\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"y\": series,\n",
    "        \"lag1\": series.shift(1),\n",
    "        \"lag2\": series.shift(2),\n",
    "        \"lag3\": series.shift(3),\n",
    "        \"hour\": series.index.hour,\n",
    "        \"dow\": series.index.dayofweek,\n",
    "        \"month\": series.index.month,\n",
    "    })\n",
    "\n",
    "    feature_cols = [\"lag1\", \"lag2\", \"lag3\", \"hour\", \"dow\", \"month\"]\n",
    "    train_df = df.dropna(subset=[\"y\"] + feature_cols)\n",
    "\n",
    "    if len(train_df) < 30:\n",
    "        # fallback to interpolation if not enough history\n",
    "        interp = series.interpolate(method=\"time\", limit_direction=\"both\")\n",
    "        interp = interp.clip(lower=cfg.min_value_mm, upper=cfg.max_value_mm)\n",
    "        labels.loc[series.isna() & interp.notna()] = \"time_interp\"\n",
    "        return interp, labels\n",
    "\n",
    "    model = HistGradientBoostingRegressor(max_depth=3, learning_rate=0.1, random_state=random_state)\n",
    "    model.fit(train_df[feature_cols], train_df[\"y\"])\n",
    "\n",
    "    filled = series.copy()\n",
    "    missing_positions = [pos for pos, value in enumerate(filled.values) if np.isnan(value)]\n",
    "\n",
    "    for pos in missing_positions:\n",
    "        ts = filled.index[pos]\n",
    "        lags = [\n",
    "            filled.iloc[pos - 1] if pos - 1 >= 0 else np.nan,\n",
    "            filled.iloc[pos - 2] if pos - 2 >= 0 else np.nan,\n",
    "            filled.iloc[pos - 3] if pos - 3 >= 0 else np.nan,\n",
    "        ]\n",
    "        if any(pd.isna(val) for val in lags):\n",
    "            continue\n",
    "        features = [[lags[0], lags[1], lags[2], ts.hour, ts.dayofweek, ts.month]]\n",
    "        pred = float(model.predict(features)[0])\n",
    "        pred = float(np.clip(pred, cfg.min_value_mm, cfg.max_value_mm))\n",
    "        filled.iloc[pos] = pred\n",
    "        labels.iloc[pos] = \"gbm_forecast\"\n",
    "\n",
    "    remaining = filled.isna()\n",
    "    if remaining.any():\n",
    "        fallback = train_df[\"y\"].median()\n",
    "        if pd.isna(fallback):\n",
    "            fallback = cfg.min_value_mm\n",
    "        filled.loc[remaining] = fallback\n",
    "        labels.loc[remaining] = \"global_median\"\n",
    "\n",
    "    return filled, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecdf64",
   "metadata": {},
   "source": [
    "### DB Utilities (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b66975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "RAW_QUERY = \"\"\"\n",
    "SELECT rm.sensor_id,\n",
    "       rm.ts,\n",
    "       rm.value_mm,\n",
    "       rm.quality,\n",
    "       rm.variable,\n",
    "       rm.source\n",
    "FROM raw_measurements rm\n",
    "LEFT JOIN clean_measurements cm\n",
    "  ON cm.sensor_id = rm.sensor_id\n",
    " AND cm.ts = rm.ts\n",
    " AND cm.version = 1\n",
    "WHERE rm.ts >= :since\n",
    "  AND cm.id IS NULL\n",
    "  AND rm.variable = 'precipitacion'\n",
    "ORDER BY rm.sensor_id, rm.ts\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fetch_raw_measurements(engine: sa.engine.Engine, since: datetime) -> pd.DataFrame:\n",
    "    stmt = text(RAW_QUERY)\n",
    "    df = pd.read_sql(stmt, engine, params={\"since\": since})\n",
    "    if not df.empty:\n",
    "        df[\"ts\"] = pd.to_datetime(df[\"ts\"], utc=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ae383",
   "metadata": {},
   "source": [
    "## 1. Load raw JSON sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbeb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('..').resolve()\n",
    "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "\n",
    "HISTORIC_PATH = RAW_DIR / 'historic_precipitation.json'\n",
    "CURRENT_PATH = RAW_DIR / 'current_precipitation.json'\n",
    "\n",
    "with HISTORIC_PATH.open() as f:\n",
    "    historic_json = json.load(f)\n",
    "\n",
    "len(historic_json), historic_json[0].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c6991",
   "metadata": {},
   "source": [
    "### Flatten one sensor's time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bcfe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_historic_sensor(raw_json, sensor_idx=0):\n",
    "    station = raw_json[sensor_idx]\n",
    "    sensor_id = f\"vaisala_{station['codigo']}\"\n",
    "    records = []\n",
    "    for entry in station['datos']:\n",
    "        ts = pd.to_datetime(entry['fecha'], utc=True)\n",
    "        for var in entry['datos']:\n",
    "            if var['variableConsulta'] != 'precipitacion':\n",
    "                continue\n",
    "            records.append({\n",
    "                'sensor_id': sensor_id,\n",
    "                'ts': ts,\n",
    "                'value_mm': var['valor'],\n",
    "                'quality': var.get('calidad')\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def choose_sensor_with_variability(raw_json, min_range=1.0, min_nonzero_ratio=0.01):\n",
    "    best_idx = 0\n",
    "    best_score = -1.0\n",
    "    for idx, station in enumerate(raw_json):\n",
    "        values = []\n",
    "        for entry in station['datos']:\n",
    "            for var in entry['datos']:\n",
    "                if var['variableConsulta'] != 'precipitacion':\n",
    "                    continue\n",
    "                val = var['valor']\n",
    "                if val is None:\n",
    "                    continue\n",
    "                values.append(val)\n",
    "        if not values:\n",
    "            continue\n",
    "        series = pd.Series(values, dtype=float)\n",
    "        positive = series[series > 0]\n",
    "        if positive.empty:\n",
    "            continue\n",
    "        value_range = positive.max() - positive.min()\n",
    "        nonzero_ratio = len(positive) / len(series)\n",
    "        score = value_range * nonzero_ratio\n",
    "        if value_range >= min_range or score > best_score:\n",
    "            best_idx = idx\n",
    "            best_score = score\n",
    "    return best_idx\n",
    "\n",
    "sensor_idx = choose_sensor_with_variability(historic_json, min_range=2.0)\n",
    "sample_raw = flatten_historic_sensor(historic_json, sensor_idx=sensor_idx)\n",
    "\n",
    "non_zero = sample_raw[sample_raw['value_mm'] > 0]\n",
    "if not non_zero.empty:\n",
    "    start = non_zero['ts'].min() - pd.Timedelta(hours=2)\n",
    "    end = non_zero['ts'].max() + pd.Timedelta(hours=2)\n",
    "    sample_raw = sample_raw[(sample_raw['ts'] >= start) & (sample_raw['ts'] <= end)]\n",
    "\n",
    "if not sample_raw.empty:\n",
    "    sample_raw = sample_raw.sort_values('ts').reset_index(drop=True)\n",
    "\n",
    "print(f\"Selected sensor index {sensor_idx} ({sample_raw['sensor_id'].iat[0]})\")\n",
    "print(sample_raw[['ts', 'value_mm']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff2664",
   "metadata": {},
   "source": [
    "### Inject synthetic gaps to exercise the forecaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adab3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_with_gaps = sample_raw.copy()\n",
    "\n",
    "if not sample_with_gaps.empty:\n",
    "    # create a synthetic multi-hour gap around the peak event\n",
    "    gap_mask = (sample_with_gaps['ts'] >= sample_with_gaps['ts'].min() + pd.Timedelta(hours=3)) &                (sample_with_gaps['ts'] <= sample_with_gaps['ts'].min() + pd.Timedelta(hours=6))\n",
    "    if gap_mask.any():\n",
    "        sample_with_gaps.loc[gap_mask, 'value_mm'] = np.nan\n",
    "        print(f\"Injected gap of {gap_mask.sum()} points between {sample_with_gaps.loc[gap_mask, 'ts'].min()} and {sample_with_gaps.loc[gap_mask, 'ts'].max()}\")\n",
    "    else:\n",
    "        # fallback: randomly drop 10 points\n",
    "        idx = sample_with_gaps.sample(min(10, len(sample_with_gaps))).index\n",
    "        sample_with_gaps.loc[idx, 'value_mm'] = np.nan\n",
    "        print(f\"Injected gap at random indices: {idx.tolist()}\")\n",
    "else:\n",
    "    print('Sample is empty; skipping gap injection')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0423825",
   "metadata": {},
   "source": [
    "cfg = CleanerConfig()\n",
    "cleaned_sample = clean_measurements(sample_with_gaps, cfg)\n",
    "\n",
    "cleaned_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c706db",
   "metadata": {},
   "source": [
    "`imputation_method` value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = CleanerConfig()\n",
    "cleaned_sample = clean_measurements(sample_with_gaps, cfg)\n",
    "\n",
    "cleaned_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea703794",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sample['imputation_method'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32be176",
   "metadata": {},
   "source": [
    "### Flag breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf45c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_stats = pd.DataFrame({\n",
    "    'outlier_share': ((cleaned_sample['qc_flags'] & OUTLIER_FLAG) > 0).mean(),\n",
    "    'imputed_share': ((cleaned_sample['qc_flags'] & IMPUTED_FLAG) > 0).mean(),\n",
    "    'poor_quality_share': ((cleaned_sample['qc_flags'] & POOR_QUALITY_FLAG) > 0).mean(),\n",
    "}, index=[0])\n",
    "flag_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f6dfe",
   "metadata": {},
   "source": [
    "### Visual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(sample_with_gaps['ts'], sample_with_gaps['value_mm'], alpha=0.3, label='raw')\n",
    "plt.plot(cleaned_sample['ts'], cleaned_sample['value_mm'], linewidth=1.2, label='cleaned')\n",
    "plt.legend()\n",
    "plt.xlabel('timestamp')\n",
    "plt.ylabel('mm')\n",
    "plt.title('Raw vs cleaned precipitation')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2c9ae",
   "metadata": {},
   "source": [
    "## 3. Optional: pull from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(BASE_DIR / '.env')\n",
    "DATABASE_URL = os.getenv('DATABASE_URL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATABASE_URL:\n",
    "    engine = sa.create_engine(DATABASE_URL, pool_pre_ping=True, future=True)\n",
    "    cutoff = datetime.now(timezone.utc) - timedelta(hours=72)\n",
    "    raw_db = fetch_raw_measurements(engine, cutoff)\n",
    "    display(raw_db.head(100))\n",
    "    if not raw_db.empty:\n",
    "        cleaned_db = clean_measurements(raw_db, CleanerConfig())\n",
    "        display(cleaned_db.head(100))\n",
    "else:\n",
    "    print('DATABASE_URL not set; skipping DB fetch.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATABASE_URL:\n",
    "    engine = sa.create_engine(DATABASE_URL, pool_pre_ping=True, future=True)\n",
    "    cutoff = datetime.now(timezone.utc) - timedelta(hours=72)\n",
    "    raw_db = fetch_raw_measurements(engine, cutoff)\n",
    "    \n",
    "    # Filter to show only non-zero measurements\n",
    "    non_zero_raw = raw_db[raw_db['value_mm'] != 0.0]\n",
    "    display(non_zero_raw.head(100))\n",
    "    \n",
    "    if not raw_db.empty:\n",
    "        cleaned_db = clean_measurements(raw_db, CleanerConfig())\n",
    "        \n",
    "        # Filter cleaned data to show only non-zero measurements\n",
    "        non_zero_cleaned = cleaned_db[cleaned_db['value_mm'] != 0.0]\n",
    "        display(non_zero_cleaned.head(100))\n",
    "else:\n",
    "    print('DATABASE_URL not set; skipping DB fetch.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
