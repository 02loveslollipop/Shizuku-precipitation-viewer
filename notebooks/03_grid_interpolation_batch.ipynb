{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15327b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure project root is on sys.path so `services` package can be imported\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "print('Added project root to sys.path:', Path('..').resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9646e1c",
   "metadata": {},
   "source": [
    "# Batch Grid Interpolation and Upload\n",
    "This notebook builds interpolated grids for the 10 most recent timestamps from `clean_measurements`, creates contour metadata and JPEG previews, saves compressed NPZ files locally under `data/processed`, and uploads artifacts to blob storage using the ETL `GridBuilder` and `BlobUploader`.\n",
    "\n",
    "Notes:\n",
    "- Reads configuration from `services/etl/config.py` which loads `.env` (so ensure `.env` in repository root contains DB and Vercel blob credentials).\n",
    "- Respects `dry_run` by not uploading when enabled.\n",
    "- Saves local NPZ files named `grid_<TIMESTAMP>.npz` in `data/processed`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure project root .env is loaded for DB + blob credentials\n",
    "load_dotenv(Path('..') / '.env')\n",
    "\n",
    "BASE_DIR = Path('..').resolve()\n",
    "PROCESSED_DIR = BASE_DIR / 'data' / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Base dir:', BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ETL helper classes\n",
    "from services.etl.config import load as load_config\n",
    "from services.etl.grid_builder import GridBuilder, GridArtifacts\n",
    "from services.etl.uploader import BlobUploader\n",
    "from services.etl.db import Database\n",
    "from services.etl.contours import generate_contours_geojson\n",
    "\n",
    "cfg = load_config()\n",
    "# Override dry_run to actually perform uploads when running this notebook interactively.\n",
    "# Remove or comment this line if you want to test without uploading.\n",
    "cfg.dry_run = False\n",
    "print('Config loaded. dry_run =', cfg.dry_run)\n",
    "\n",
    "uploader = BlobUploader(cfg)\n",
    "builder = GridBuilder(res_m=cfg.grid_resolution_m, padding_m=cfg.grid_padding_m)\n",
    "db = Database(cfg)\n",
    "\n",
    "# Use the Database engine for any direct SQL access if needed\n",
    "engine = db.engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d6733",
   "metadata": {},
   "source": [
    "## Fetch 10 most recent timestamps\n",
    "We query `clean_measurements` grouping by timestamp and select the 10 most recent timestamps by count or simply by timestamp value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c31a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force mode: set to True to bypass `grid_runs` pending-slot logic and process the\n",
    "# top-N timestamps by highest mean value directly. Use with caution â€” DB status\n",
    "# updates will be performed for forced entries by creating grid_runs rows if they\n",
    "# don't already exist.\n",
    "FORCE = True\n",
    "FORCE_TOP_N = 10\n",
    "\n",
    "\n",
    "def _get_or_create_run_for_slot(slot_ts: pd.Timestamp, conn) -> int:\n",
    "    \"\"\"Return existing grid_runs.id for (ts, res_m) or insert a new pending row and return its id.\"\"\"\n",
    "    # Try to find an existing run for the exact slot timestamp and resolution\n",
    "    sel = sa.text(\"SELECT id FROM grid_runs WHERE ts = :ts AND res_m = :res_m LIMIT 1\")\n",
    "    row = conn.execute(sel, {\"ts\": slot_ts, \"res_m\": cfg.grid_resolution_m}).fetchone()\n",
    "    if row and row[0] is not None:\n",
    "        return int(row[0])\n",
    "    # Insert a new pending run\n",
    "    ins = sa.text(\n",
    "        \"INSERT INTO grid_runs (ts, res_m, bbox, crs, status, created_at, updated_at) \"\n",
    "        \"VALUES (:ts, :res_m, '[]'::jsonb, 'EPSG:3857', 'pending', NOW(), NOW()) RETURNING id\"\n",
    "    )\n",
    "    new = conn.execute(ins, {\"ts\": slot_ts, \"res_m\": cfg.grid_resolution_m}).fetchone()\n",
    "    return int(new[0])\n",
    "\n",
    "\n",
    "if FORCE:\n",
    "    print('FORCE mode enabled: fetching top', FORCE_TOP_N, 'timestamps by mean value')\n",
    "    qry = sa.text('''\n",
    "        SELECT ts AT TIME ZONE 'UTC' AS ts_utc\n",
    "        FROM (\n",
    "            SELECT ts, AVG(value_mm) as mean_mm\n",
    "            FROM clean_measurements\n",
    "            GROUP BY ts\n",
    "            ORDER BY mean_mm DESC\n",
    "            LIMIT :limit\n",
    "        ) t\n",
    "        ORDER BY mean_mm DESC\n",
    "    ''')\n",
    "    rows = pd.read_sql(qry, engine, params={'limit': FORCE_TOP_N})\n",
    "    rows['ts_utc'] = pd.to_datetime(rows['ts_utc'], utc=True)\n",
    "\n",
    "    # Create or find grid_runs entries for each forced timestamp so we can update DB status\n",
    "    forced = []\n",
    "    with db.engine.begin() as conn:\n",
    "        for ts in rows['ts_utc'].tolist():\n",
    "            run_id = _get_or_create_run_for_slot(ts, conn)\n",
    "            forced.append((run_id, ts))\n",
    "\n",
    "    pending_slots = forced\n",
    "    print('Created/located', len(pending_slots), 'grid_runs entries for forced timestamps')\n",
    "    for rid, ts in pending_slots:\n",
    "        print(rid, ts)\n",
    "else:\n",
    "    # Use the DB helper to ensure slots exist and fetch pending slots to process\n",
    "    # This mirrors the ETL service behavior and ensures the SQL table `grid_runs` is updated.\n",
    "    print('Ensuring slots...')\n",
    "    db.ensure_slots()\n",
    "\n",
    "    pending = db.fetch_pending_slots()\n",
    "    # pending is a list of (run_id, timestamp)\n",
    "    print('Found pending slots:', len(pending))\n",
    "    for rid, ts in pending:\n",
    "        print(rid, ts)\n",
    "\n",
    "    # Expose ids and timestamps for the loop below\n",
    "    pending_slots = pending\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d286ab",
   "metadata": {},
   "source": [
    "## Helper: load snapshot for a timestamp\n",
    "We reuse the same 5-minute window approach as the single-file notebook to gather sensor measurements around each timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d57e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def ensure_utc(value) -> pd.Timestamp:\n",
    "    ts = pd.Timestamp(value)\n",
    "    if ts.tzinfo is None:\n",
    "        return ts.tz_localize('UTC')\n",
    "    return ts.tz_convert('UTC')\n",
    "\n",
    "def load_clean_snapshot(target_ts, window_minutes=5):\n",
    "    ts = ensure_utc(target_ts)\n",
    "    start = ts - pd.Timedelta(minutes=window_minutes)\n",
    "    end = ts + pd.Timedelta(minutes=window_minutes)\n",
    "    query = sa.text(\"\"\"\n",
    "        SELECT cm.sensor_id, cm.ts, cm.value_mm, cm.imputation_method,\n",
    "               s.lat, s.lon\n",
    "        FROM clean_measurements cm\n",
    "        JOIN sensors s ON s.id = cm.sensor_id\n",
    "        WHERE cm.ts BETWEEN :start AND :end\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, engine, params={'start': start, 'end': end})\n",
    "    df['ts'] = pd.to_datetime(df['ts'], utc=True)\n",
    "    return df.sort_values('sensor_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c4454",
   "metadata": {},
   "source": [
    "## Loop over timestamps: build + save + upload\n",
    "For each timestamp we will build the grid, save a local NPZ, upload the NPZ, upload a gzipped JSON grid payload via `upload_grid_json`, and upload the JPEG preview if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ea2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Helper to save NPZ\n",
    "def save_npz(local_path: Path, artifacts: GridArtifacts):\n",
    "    np.savez_compressed(\n",
    "        local_path,\n",
    "        data=artifacts.data_grid.astype(np.float32),\n",
    "        x=artifacts.x_coords.astype(np.float64),\n",
    "        y=artifacts.y_coords.astype(np.float64),\n",
    "        metadata=artifacts.metadata_json,\n",
    "    )\n",
    "\n",
    "# Loop over pending_slots (run_id, slot)\n",
    "processed: list = []\n",
    "errors: list = []\n",
    "for run_id, slot in pending_slots:\n",
    "    ts = slot\n",
    "    print('---')\n",
    "    print('Processing slot', slot, 'id', run_id)\n",
    "    try:\n",
    "        snapshot = db.load_snapshot(slot)\n",
    "        if snapshot.empty:\n",
    "            raise ValueError('no clean measurements for slot')\n",
    "\n",
    "        artifact = builder.build(snapshot)\n",
    "\n",
    "        timestamp = slot.strftime('%Y%m%dT%H%M%SZ')\n",
    "        base_key = f'grids/{timestamp}'\n",
    "        local_dir = PROCESSED_DIR / timestamp\n",
    "        local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save local NPZ\n",
    "        local_npz = local_dir / 'grid.npz'\n",
    "        save_npz(local_npz, artifact)\n",
    "        print('Saved local NPZ to', local_npz)\n",
    "\n",
    "        if cfg.dry_run:\n",
    "            print('dry-run: would upload artifacts for slot', slot, 'grid shape', artifact.data_grid.shape)\n",
    "            db.mark_success(run_id, json.dumps(list(artifact.bbox_3857)), npz_url='', json_url='', contours_url='', message='dry-run')\n",
    "            processed.append(timestamp)\n",
    "            continue\n",
    "\n",
    "        # Upload NPZ using uploader.upload_npz which expects a dict of arrays\n",
    "        npz_payload = {\n",
    "            'data': artifact.data_grid.astype(np.float32),\n",
    "            'x': artifact.x_coords.astype(np.float64),\n",
    "            'y': artifact.y_coords.astype(np.float64),\n",
    "            'metadata': np.array([artifact.metadata_json]),\n",
    "        }\n",
    "        npz_url = uploader.upload_npz(f'{base_key}/grid.npz', npz_payload)\n",
    "        print('Uploaded NPZ ->', npz_url)\n",
    "\n",
    "        # Upload grid json gzip\n",
    "        grid_json_url = uploader.upload_grid_json(f'{base_key}/grid.json.gz', artifact)\n",
    "        print('Uploaded grid JSON gzip ->', grid_json_url)\n",
    "\n",
    "        # Generate and upload contours geojson\n",
    "        contour_bytes = generate_contours_geojson(artifact.x_coords, artifact.y_coords, artifact.data_grid, artifact.thresholds)\n",
    "        contours_url = uploader.upload_bytes(f'{base_key}/contours.geojson', contour_bytes, 'application/geo+json')\n",
    "        print('Uploaded contours ->', contours_url)\n",
    "\n",
    "        # Upload JPEG preview if available\n",
    "        jpeg_url = None\n",
    "        if getattr(artifact, 'jpeg_bytes', None):\n",
    "            try:\n",
    "                jpeg_url = uploader.upload_bytes(f'{base_key}/preview.jpg', artifact.jpeg_bytes, 'image/jpeg')\n",
    "                print('Uploaded JPEG preview ->', jpeg_url)\n",
    "            except Exception:\n",
    "                jpeg_url = None\n",
    "\n",
    "        # Update grids/latest.json pointer\n",
    "        metadata = json.loads(artifact.metadata_json)\n",
    "        latest_payload = {\n",
    "            'timestamp': metadata['timestamp'],\n",
    "            'grid_npz_url': npz_url,\n",
    "            'grid_json_url': grid_json_url,\n",
    "            'grid_preview_jpeg_url': jpeg_url,\n",
    "            'contours_url': contours_url,\n",
    "            'res_m': cfg.grid_resolution_m,\n",
    "            'bbox': metadata['bbox_wgs84'],\n",
    "            'intensity_classes': metadata.get('intensity_classes', []),\n",
    "            'intensity_thresholds': metadata.get('intensity_thresholds', []),\n",
    "        }\n",
    "        latest_url = uploader.upload_json('grids/latest.json', latest_payload)\n",
    "        print('Updated latest pointer ->', latest_url)\n",
    "\n",
    "        # Mark success in DB\n",
    "        db.mark_success(run_id, json.dumps(list(artifact.bbox_3857)), npz_url=npz_url, json_url=grid_json_url, contours_url=contours_url)\n",
    "\n",
    "        processed.append(timestamp)\n",
    "        print('slot processed:', slot.isoformat(), base_key)\n",
    "\n",
    "    except Exception as exc:\n",
    "        print('slot failed:', slot, exc)\n",
    "        db.mark_failure(run_id, str(exc))\n",
    "        errors.append({'run_id': run_id, 'timestamp': str(slot), 'error': str(exc)})\n",
    "\n",
    "print('Done. processed:', processed)\n",
    "print('errors:', errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d2892e",
   "metadata": {},
   "source": [
    "## Result\n",
    "The notebook created and uploaded (unless `dry_run` is set) the grids. Check `data/processed` for saved `.npz` files and your Vercel blob storage for uploaded artifacts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siata-viewer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
